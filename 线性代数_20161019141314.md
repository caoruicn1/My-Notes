##Chapter 1 Introduction to Vectors
* **Vector addition:** $v+w$ and 
* **linear combinations**: $Ax_1+Bx_2=b$* **The dot product:** $v·w$ of two vectors
	* **Perpendicular vectors**: Dot product is zero.
	* **The length** = $||v||= \sqrt{v·v} $
		* **A unit vector**: $u = \frac{v}{||v||}$  length = 1,
* **Fomular**
	* $\frac{v·w}{||v||\ ||w||} = cos \theta$
	* Schwar inequality: $|v·w| \leq ||v||\ ||w||$
	* Triangle inequality: $||v+w|| \leq ||v||+||w||$
* **linear equations** $Ax = b$, **solutions** $x = A^{-1}b$. when A is invertible
* **The Inverse Matrix**: $AA^{-1}=I$
	* **Independent** columns: $Ax=0$ has one solution. $A$ is an **invertible** matrix
	* **Dependent** columns: $Ax = 0$ has many solutions. $A$ is a **singular** matrix.
* **Expample**: If there are 9 columns, but not all of them are independent and it's actually 8 columns. Then they will not cover all the 9 dimensions, then not all b have solutions.
* All solutions of a linear equation is a plane

* **Two ways to think** $Ax=b$
	1. Dot product
	2. combination of the columns of the matrix


##Chapter 2 Solving Linear Equations
### Vectors and Linear Equations

* **Coefficient matrix**: On the left side of the equations
* $A = LU =(lower\ triangle)(upper\ triangle)$
	* Elimination goes from $A$ to a triangular $U$ by a sequence of matrix steps   $E_{ij}$.
	* The inverse matrices $E_{ij}^{-1}$ in **reverse** order bring $U$ back to the original $A$.

###  The Idea of Elimination
* **Pivot**: first nonzero in the row that does the elimination
* **Multiplier**: $l_{ij}=\frac{entry\ to\ eliminate\ in\ row\ i}{pibot\ in\ row\ j}$

* **Singular**：Just one pivot. $Determinent = 0$
* **Nonsingular**: A full set of pivots and One solution.

* **Identity matrix**: $I = \begin{bmatrix}1 & 0 & 0 \\ 0 & 1 & 0\\0&0&1\end{bmatrix} $

### Elimination Using Matrices

* **Matrix Multiplication**
	* Associative law is **true**: $A(BC)=(AB)C$
	* Commutative law is **false**: Often $AB \neq BA$

* **Elimination matrix**: Subtract row j from row i we use $E_{ij}$
* **Permutation matrix**: Exchange rows we use $P_{ij}$
	* To exchange equations 1 and 3 multiply by $P_{13}=\begin{bmatrix}0&0&1 \\ 0&1&0 \\ 1&0&0 \end{bmatrix}$

* **Augmented matrix**: $ [A\ b] = \begin{bmatrix}2&4&-2&2 \\ 4&9&-3&8 \\ -2&-3&7&10 \end{bmatrix}$

* **Rules for Matrix Operation**: If $A$ has $n$ columns, $B$ must have $n$ rows.

* To do row operations, it multiplies on the left
* To do column operation, the matrix multiplies on the right

* **Block multiplication** is allowed when the block shapes match correctly.
* **Block elimination** produces the *Schur complement* $D-CA^{-1}B$

### Inverse Matrices 
* 
	* If invertible, exist $A^{-1}A=I$ and $AA^{-1}=I$
	* The **inverse exists** if and only if elimination produces **n pivots**
	* If $A$ is invertble, the solution of $Ax=b$ is $x=A^{-1}b$
	* If $A$ is invertible, then $Ax=0$ can only have zero solution $x=A^{-1}0=0$
	* 2 by 2 Inverse: $\begin{bmatrix}a&b \\ c&d \end{bmatrix}^{-1} = \frac{1}{ad-bc}\begin{bmatrix}d&-b \\ -c&a \end{bmatrix}^{-1}$ 
	* If $A = \begin{bmatrix}d_1& \\ &\ddots \\ &&d_n\end{bmatrix}^{-1}$, then $A^{-1}= \begin{bmatrix}\frac{1}{d_1}& \\ &\ddots \\ &&\frac{1}{d_n}\end{bmatrix}^{-1}$
	* $(AB)^{-1}=B^{-1}A^{-1}$ because $ABB^{-1} A^{-1} = A(BB^{-1})A^{-1}=I$
	* Reverse order: $(ABC)^{-1}=C^{-1}B^{-1}A^{-1}$
	* $E^{-1}F^{-1}$ is quick and more beautiful than directly $FE$

* **Calculating Inverse：**
	* Gauss—Jordan Elimination：$[A\ I] = [I\ A^{-1}]$, because $E[A\ I] = [I\ A^{-1}]$
	
### Elimination = Factorization: $A = LU$

* 
	* No row exchange, The factors L and U are **triangular matrices**.
	* The lower triangular L contains the numbers $l_{ij}$ that multiply pivot rows, going from $A$ to $U$ . The product $LU$ adds those rows back to recover $A$.
	* L include all inverses of $E_{ij}$ in opposite order.
$$
\begin{align}
E_{32}E_{31}E_{21}A & = U \\ 
 A & = (E_{21}^{-1}E_{31}^{-1}E_{32}^{-1})U \\ A & =LU
\end{align}
$$
	* When a **row/column** of $A$ starts with **zeros**, so does that row of **$L$ / $U$**.
	* $A=LDU$, D is **diagonal** and U has **1's** on the diagonal.

* **One Square System = Two Triangular Systems**
	* One Square System:
		1. Factor (into L and U, by elimination on the left side matrix A)
		2. Solve (forward elimination on $b$ using $L$, then back subsititution for $x$ using $U$)
	* Two Triangular Systems
		* Solve $Lc=b$ (forward), and then solve $Ux=c$ (backward)

* **The Cost of Elimination**
	* **Factor**: Elimination on $A$ requiares about $\frac{1}{3}n^3$ multiplications and $\frac{1}{3}n^3$ subtractions.
	* **Solve**: There are n2 multiplications and subtractions on the right side.
	* **Band matrix**: A band matrix has only $w$ nonzero diagonals below and also above its main diagonal.
		* For a band matrix, change $\frac{1}{3}n^3$ to $nw^2$ and change $n^2$ to $2wn$.
	* If a zero shows up in the pivot position, it will need **row exchange**.

### Transposes and Permutations


